{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a88baab",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "csv_file_path = os.getenv('HOME')+'/aiffel/data_preprocess/data/trade.csv'\n",
    "trade = pd.read_csv(csv_file_path) \n",
    "trade.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73214efc",
   "metadata": {},
   "source": [
    "print('전체 데이터 건수:', len(trade))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4145ec63",
   "metadata": {},
   "source": [
    "print('컬럼별 결측치 개수')\n",
    "len(trade) - trade.count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2e7c45",
   "metadata": {},
   "source": [
    "trade = trade.drop('기타사항', axis=1)\n",
    "trade.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "27a443a0",
   "metadata": {},
   "source": [
    "`DataFrame.isnull()`은 데이터마다 결측치 여부를 True, False로 반환합니다. `DataFrame.any(axis=1)`는 행마다 하나라도 True가 있으면 True, 그렇지 않으면 False를 반환합니다.\n",
    "두 메서드를 조합하여 결측치가 하나라도 있는 행을 찾아보겠습니다.\n",
    "\n",
    "`DataFrame에 isnull()`을 적용하고, 여기도 또 any(axis=1) 메서드를 적용합니다. 이 결과, '각 행이 결측치가 하나라도 있는지' 여부를 불리언 값으로 가진 Series가 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57386c8e",
   "metadata": {},
   "source": [
    "trade.isnull()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b41864",
   "metadata": {},
   "source": [
    "trade.isnull().any(axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92243239",
   "metadata": {},
   "source": [
    "trade[trade.isnull().any(axis=1)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912c320c",
   "metadata": {},
   "source": [
    "trade.dropna(how='all', subset=['수출건수', '수출금액', '수입건수', '수입금액', '무역수지'], inplace=True)\n",
    "print(\"👽 It's okay, no biggie.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758385f4",
   "metadata": {},
   "source": [
    "# Q. 결측치가 하나라도 존재하는 데이터를 다시 확인해봅시다.\n",
    "# trade.loc[trade.isnull().sum(axis=1) > 0]\n",
    "trade[trade.isnull().any(axis=1)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "11c0e78a",
   "metadata": {},
   "source": [
    "index 191과 같이 수치형 데이터를 보완할 방법은 많습니다.\n",
    "\n",
    "특정 값을 지정해 줄 수 있습니다. 그러나 결측치가 많은 경우, 모두 같은 값으로 대체한다면 데이터의 분산이 실제보다 작아지는 문제가 생길 수 있습니다.\n",
    "평균, 중앙값 등으로 대체할 수 있습니다. 1번에서 특정 값으로 대체했을 때와 마찬가지로 결측치가 많은 경우 데이터의 분산이 실제보다 작아지는 문제가 발생할 수 있습니다.\n",
    "다른 데이터를 이용해 예측값으로 대체할 수 있습니다. 예를 들어 머신러닝 모델로 2020년 4월 미국의 예측값을 만들고, 이 값으로 결측치를 보완할 수 있습니다.\n",
    "시계열 특성을 가진 데이터의 경우 앞뒤 데이터를 통해 결측치를 대체할 수 있습니다. 예를 들어 기온을 측정하는 센서 데이터에서 결측치가 발생할 경우, 전후 데이터의 평균으로 보완할 수 있습니다.\n",
    "index 191은 4번 방법을 통해 보완하도록 하겠습니다.\n",
    "\n",
    "trade 데이터셋에서 국가명인 미국이며 2020년 3월과 5월 데이터셋을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2daf58a3",
   "metadata": {},
   "source": [
    "trade[(trade['국가명']=='미국')&((trade['기간']=='2020년 03월')|(trade['기간']=='2020년 05월'))]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2400569a",
   "metadata": {},
   "source": [
    "trade.loc[191, '수출금액'] = (trade.loc[188, '수출금액'] + trade.loc[194, '수출금액'] )/2\n",
    "trade.loc[[191]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab868be2",
   "metadata": {},
   "source": [
    "trade.loc[191, '무역수지'] = trade[(trade['국가명']=='미국')&((trade['기간']=='2020년 03월')|(trade['기간']=='2020년 05월'))].mean()['무역수지']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ad3ed5",
   "metadata": {},
   "source": [
    "trade.drop(191)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e0760496",
   "metadata": {},
   "source": [
    "지금까지 데이터가 수치형인 경우 결측치를 삭제하거나 대체하는 방법을 실습해 보았습니다. 데이터가 범주형인 경우는 어떻게 해야 할까요? 이는 수치형일 때와 유사합니다.\n",
    "\n",
    "1. 특정 값을 지정해줄 수 있습니다. 예를 들어 ‘기타’, ‘결측’과 같이 새로운 범주를 만들어 결측치를 채울 수 있습니다.\n",
    "1. 최빈값 등으로 대체합니다. 결측치가 많은 경우 최빈값이 지나치게 많아질 수 있으므로 결측치가 많을 때는 다른 방법을 사용하는 것이 좋습니다.\n",
    "1. 다른 데이터를 이용해 예측값으로 대체할 수 있습니다.\n",
    "1. 시계열 특성을 가진 데이터의 경우 앞뒤 데이터를 통해 결측치를 대체할 수 있습니다. 예를 들어 특정인의 2019년 직업이 결측치이고, 2018년과 2020년 직업이 일치한다면 그 값으로 보완할 수 있습니다. 만약 다르다면 둘 중 하나로 보완하도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73064501",
   "metadata": {},
   "source": [
    "## 중복된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da4f14c",
   "metadata": {},
   "source": [
    "trade.duplicated()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf74151",
   "metadata": {},
   "source": [
    "trade[trade.duplicated()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab15313",
   "metadata": {},
   "source": [
    "trade[(trade['기간']=='2020년 03월')&(trade['국가명']=='중국')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec676dc",
   "metadata": {},
   "source": [
    "trade.drop_duplicates(inplace=True)\n",
    "print(\"👽 It's okay, no biggie.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078091e6",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({'id':['001', '002', '003', '004', '002'], \n",
    "                   'name':['Park Yun', 'Kim Sung', 'Park Jin', 'Lee Han', 'Kim Min']})\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "51eb20ce",
   "metadata": {},
   "source": [
    "id가 002인 데이터가 2개 있습니다. id가 사람마다 unique 하다고 할 때, 둘 중 하나는 삭제해야 합니다.\n",
    "index가 클수록 나중에 들어온 데이터이고, 사용자가 이름을 수정했을 때 업데이트가 되지 않고 삽입이 되어 생긴 문제라고 가정합니다. 즉, id가 중복된 경우 맨 나중에 들어온 값만 남겨야 합니다.\n",
    "\n",
    "DataFrame.drop_duplicates의 subset, keep 옵션을 통해 손쉽게 중복을 제거할 수 있습니다.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dee44",
   "metadata": {},
   "source": [
    "# Q. 링크의 공식 문서를 참고해서,\n",
    "# id가 중복된 경우 나중에 들어온 값만 남기는 코드를 작성해보세요!\n",
    "# ('Kim Sung' 항목이 삭제되어야 합니다.)\n",
    "\n",
    "df.drop_duplicates(subset='id', keep='last')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a4920dac",
   "metadata": {},
   "source": [
    "## 이상치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d637c0f",
   "metadata": {},
   "source": [
    "trade 데이터에서 큰 값을 가지는 이상치가 있다고 가정해 봅시다. 이상치란 대부분 값의 범위에서 벗어나 극단적으로 크거나 작은 값을 의미합니다. Min-Max Scaling 해보면 대부분의 값은 0에 가깝고 이상치만 1에 가까운 값을 가지게 될 것입니다. 이렇게 몇 개의 이상치 때문에 대부분 값의 차이는 의미가 거의 없어지게 됩니다. 극단적인 값이 생기는 경우를 제외하고 데이터를 고려하고 싶은 경우 이상치를 제거하고 분석합니다.\n",
    "\n",
    "그렇다면 이상치를 어떻게 찾아내야 할까요? 현실에서 이상치를 찾는 것(anomaly detection) 자체가 큰 분야입니다.\n",
    "\n",
    "가장 먼저 생각해 볼 수 있는 간단하고 자주 사용되는 방법은 평균과 표준편차를 이용하는 z score 방법입니다.\n",
    "평균을 빼주고 표준편차로 나눠 z score를 계산합니다. 그리고 z score가 특정 기준을 넘어서는 데이터에 대해 이상치라고 판단합니다. 기준을 작게 하면 이상치라고 판단하는 데이터가 많아지고, 기준을 크게 하면 이상치라고 판단하는 데이터가 적어집니다.\n",
    "\n",
    "이상치를 판단한 뒤 어떻게 해야 할까요?\n",
    "\n",
    "가장 간단한 방법으로 이상치를 삭제할 수 있습니다. 이상치를 원래 데이터에서 삭제하고, 이상치끼리 따로 분석하는 방안도 있습니다.\n",
    "이상치를 다른 값으로 대체할 수 있습니다. 데이터가 적으면 이상치를 삭제하기보다 다른 값으로 대체하는 것이 나을 수 있습니다. 예를 들어 최댓값, 최솟값을 설정해 데이터의 범위를 제한할 수 있습니다.\n",
    "혹은 결측치와 마찬가지로 다른 데이터를 활용하여 예측 모델을 만들어 예측값을 활용할 수도 있습니다.\n",
    "아니면 binning을 통해 수치형 데이터를 범주형으로 바꿀 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26190e9",
   "metadata": {},
   "source": [
    "### z-score method\n",
    "이제 우리 데이터를 처리해 봅시다. 이상치인 데이터의 인덱스를 리턴하는 outlier라는 함수를 만들었습니다. 데이터프레임 df, 컬럼 col, 기준 z를 인풋으로 받습니다.\n",
    "\n",
    "- `abs(df[col] - np.mean(df[col]))` : 데이터에서 평균을 빼준 것에 절대값을 취합니다.\n",
    "- `abs(df[col] - np.mean(df[col]))/np.std(df[col])` : 위에서 얻은 값을 표준편차로 나눠줍니다.\n",
    "- `df[abs(df[col] - np.mean(df[col]))/np.std(df[col])>z].index` : 값이 z보다 큰 데이터의 인덱스를 추출합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b73f7c7d",
   "metadata": {},
   "source": [
    "def outlier(df, col, z):\n",
    "    return df[abs(df[col] - np.mean(df[col]))/ np.std(df[col]) > z].index\n",
    "print(\"👽 It's okay, no biggie.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c572c8c2",
   "metadata": {},
   "source": [
    "trade.loc[outlier(trade, '무역수지', 1.5)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05021b39",
   "metadata": {},
   "source": [
    "# Q. 무역수지 값을 기준으로 z=2일 때 이상치 데이터를 출력해보세요.\n",
    "trade.loc[outlier(trade, '무역수지', 2)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af6aeed",
   "metadata": {},
   "source": [
    "# Q. 무역수지 값을 기준으로 z=3일 때 이상치 데이터를 출력해보세요.\n",
    "trade.loc[outlier(trade, '무역수지', 3)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5c7ccc13",
   "metadata": {},
   "source": [
    "무역수지의 이상치를 확인하는데 기준 되는 값이 클수록 이상치가 적어지는 것을 확인할 수 있습니다.   \n",
    "이제 `not_outlier`라는 함수를 통해 무역수지가 이상치 값이 아닌 데이터만 추출하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d80c04",
   "metadata": {},
   "source": [
    "def not_outlier():\n",
    "    return df[abs(df[col] - np.mean(df[col]))/np.std(df[col])<=z].index"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330dc69",
   "metadata": {},
   "source": [
    "trade.loc[not_outlier(trade, '무역수지', 1.5)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b22f1a23",
   "metadata": {},
   "source": [
    "### IQR method\n",
    "하지만 이상치를 찾는 방법에는 위에 설명한 z-score 방법만 있는 것은 아닙니다. 그리고 z-score 방법은 몇 가지 뚜렷한 한계점을 가지고 있습니다. z-score 방법의 대안으로 사분위 범위수 IQR(Interquartile range) 로 이상치를 알아내는 방법을 알아보겠습니다.\n",
    "\n",
    "이해를 돕기 위해 아웃라이어가 포함된 임의의 데이터를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d36d34cd",
   "metadata": {},
   "source": [
    "np.random.seed(2020)\n",
    "data = np.random.randn(100)  # 평균 0, 표준편차 1의 분포에서 100개의 숫자를 샘플링한 데이터 생성\n",
    "data = np.concatenate((data, np.array([8, 10, -3, -5])))      # [8, 10, -3, -5])를 데이터 뒤에 추가함\n",
    "data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cffe209",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(data)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5451d24e",
   "metadata": {},
   "source": [
    "우리는 사분위 범위수 IQR(Interquartile range)을 이용하여 이상치를 찾아낼 수 있습니다.\n",
    "\n",
    "IQR = Q_3 - Q_1\n",
    " \n",
    "즉, IQR은 제 3사분위수에서 제 1사분위 값을 뺀 값으로 데이터의 중간 50%의 범위라고 생각하시면 됩니다. \n",
    "Q_1 - 1.5 * IQR보다 왼쪽에 있거나 Q_3 + 1.5 * IQR보다 오른쪽에 있는 경우 우리는 이상치라고 판단합니다.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Interquartile_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7764689c",
   "metadata": {},
   "source": [
    "Q3, Q1 = np.percentile(data, [75 ,25])\n",
    "IQR = Q3 - Q1\n",
    "IQR"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99513c5c",
   "metadata": {},
   "source": [
    "data[(Q1-1.5*IQR > data)|(Q3+1.5*IQR < data)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "141fa3ce",
   "metadata": {},
   "source": [
    "https://modulabs.co.kr/blog/outlier-detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49a833",
   "metadata": {},
   "source": [
    "#### z-score 방법이 가지는 뚜렷한 단점 2가지\n",
    "① Robust하지 못합니다. 왜나하면 평균과 표준편차 자체가 이상치의 존재에 크게 영향을 받기 때문입니다.    \n",
    "② 작은 데이터셋의 경우 z-score의 방법으로 이상치를 알아내기 어렵습니다. 특히 item이 12개 이하인 데이터셋에서는 불가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c35a004",
   "metadata": {},
   "source": [
    "# Q. 사분위 범위수를 이용해서 이상치를 찾는 outlier2() 함수를 구현해보세요.\n",
    "def outlier2(df, col):\n",
    "    Q3, Q1 = np.percentile(df[col], [75 ,25])\n",
    "    IQR = Q3 - Q1\n",
    "    return df[(Q1-1.5*IQR > df[col])|(Q3+1.5*IQR < df[col])]\n",
    "\n",
    "outlier2(trade, '무역수지')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "80596fe3",
   "metadata": {},
   "source": [
    "## 정규화(Normalization)\n",
    "trade 데이터를 보면 수입건수, 수출건수와 수입금액, 수출금액, 무역수지는 단위가 다르다는 것을 알 수 있습니다.\n",
    "\n",
    "이처럼 컬럼마다 스케일이 크게 차이가 나는 데이터를 입력하면 머신러닝 모델 학습에 문제가 발생할 수 있습니다. 예를 들어 데이터의 범위가 0에서 1 사이인 컬럼 A과 1000에서 10000 사이인 컬럼 B가 있다고 생각해 봅시다. 이런 데이터를 클러스터링 한다고 가정해 봅시다. 데이터 간의 거리를 잴 때, 범위가 큰 컬럼 B의 값에만 영향을 크게 받을 것입니다. 다른 예시로 간단한 linear regression을 한다고 가정해 봅시다. 모델의 파라미터를 업데이트하는 과정에서 범위가 큰 컬럼 B의 파라미터만 집중적으로 업데이트하는 문제가 생길 수 있습니다. 그래서 일반적으로 컬럼 간에 범위가 크게 다를 경우 전처리 과정에서 데이터를 정규화합니다.\n",
    "\n",
    "정규화를 하는 방법은 다양하지만, 가장 잘 알려진 표준화(Standardization)와 Min-Max Scaling을 알아보도록 하겠습니다.\n",
    "\n",
    "### Standardization\n",
    "- 데이터의 평균은 0, 분산은 1로 변환합니다.\n",
    "- Standardization은 보통 평균이 0이고 표준편차가 1일 때 사용합니다. 그렇기에 데이터가 가우시안 분포를 따를 경우 유용합니다.\n",
    "     \n",
    "$$X- \\mu \\over \\sigma$$\n",
    "\n",
    "### Min-Max Scaling\n",
    "- 데이터의 최솟값은 0, 최댓값은 1로 변환합니다.\n",
    "- Min-Max Scaling은 피처의 범위가 다를 때 주로 사용하며 확률 분포를 모를 때 유용합니다.\n",
    "    \n",
    "$$X- X_{min} \\over X_{max} - X_{min}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44e4159e",
   "metadata": {},
   "source": [
    "# 정규분포를 따라 랜덤하게 데이터 x를 생성합니다. \n",
    "x = pd.DataFrame({'A': np.random.randn(100)*4+4,\n",
    "                 'B': np.random.randn(100)-1})\n",
    "x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5162a382",
   "metadata": {},
   "source": [
    "# 데이터 x를 Standardization 기법으로 정규화합니다. \n",
    "x_standardization = (x-x.mean())/x.std()\n",
    "x_standardization"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c750f123",
   "metadata": {},
   "source": [
    "# 데이터 x를 min-max scaling 기법으로 정규화합니다. \n",
    "x_min_max = (x-x.min())/(x.max()-x.min())\n",
    "x_min_max"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6a1222c",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12, 4),\n",
    "                        gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "axs[0].scatter(x['A'], x['B'])\n",
    "axs[0].set_xlim(-5, 15)\n",
    "axs[0].set_ylim(-5, 5)\n",
    "axs[0].axvline(c='grey', lw=1)\n",
    "axs[0].axhline(c='grey', lw=1)\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "axs[1].scatter(x_standardization['A'], x_standardization['B'])\n",
    "axs[1].set_xlim(-5, 5)\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].axvline(c='grey', lw=1)\n",
    "axs[1].axhline(c='grey', lw=1)\n",
    "axs[1].set_title('Data after standardization')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3999e90e",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12, 4),\n",
    "                        gridspec_kw={'width_ratios': [2, 1]})\n",
    "\n",
    "axs[0].scatter(x['A'], x['B'])\n",
    "axs[0].set_xlim(-5, 15)\n",
    "axs[0].set_ylim(-5, 5)\n",
    "axs[0].axvline(c='grey', lw=1)\n",
    "axs[0].axhline(c='grey', lw=1)\n",
    "axs[0].set_title('Original Data')\n",
    "\n",
    "axs[1].scatter(x_min_max['A'], x_min_max['B'])\n",
    "axs[1].set_xlim(-5, 5)\n",
    "axs[1].set_ylim(-5, 5)\n",
    "axs[1].axvline(c='grey', lw=1)\n",
    "axs[1].axhline(c='grey', lw=1)\n",
    "axs[1].set_title('Data after min-max scaling')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "60a8b1dc",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "우선 정규화를 시켜야 할 수치형 컬럼들을 cols 변수에 담은 후, 데이터에서 평균을 빼고, 표준편차로 나눠주도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "017a648b",
   "metadata": {},
   "source": [
    "# trade 데이터를 standardization 기법으로 정규화합니다. \n",
    "cols = ['수출건수', '수출금액', '수입건수', '수입금액', '무역수지']\n",
    "trade_standardization = (trade[cols]-trade[cols].mean())/trade[cols].std()\n",
    "trade_standardization.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0c54e63a",
   "metadata": {},
   "source": [
    "standardization 방법으로 정규화시킨 trade_standardization을 확인해 보겠습니다. 각 컬럼의 평균들을 보면 거의 0에 가깝고, 표준편차는 1에 가까운 것을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50107b22",
   "metadata": {},
   "source": [
    "trade_standardization.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4af5fdb5",
   "metadata": {},
   "source": [
    "### Min-Max Scaling\n",
    "데이터에서 최솟값을 빼주고, '최댓값-최솟값'으로 나눠줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87a72601",
   "metadata": {},
   "source": [
    "(trade[cols] - trade[cols].min()) / (trade[cols].max() - trade[cols].min())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22db3cc7",
   "metadata": {},
   "source": [
    "# Q. trade 데이터를 min-max scaling 기법으로 정규화합니다.\n",
    "trade[cols] = (trade[cols] - trade[cols].min()) / (trade[cols].max() - trade[cols].min())\n",
    "trade.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2d59f76",
   "metadata": {},
   "source": [
    "trade.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77b1c4ef",
   "metadata": {},
   "source": [
    "train 데이터와 test 데이터가 나눠져 있는 경우 train 데이터를 정규화시켰던 기준 그대로 test 데이터도 정규화시켜줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe79b24b",
   "metadata": {},
   "source": [
    "train = pd.DataFrame([[10, -10], [30, 10], [50, 0]])\n",
    "test = pd.DataFrame([[0, 1], [10, 10]])\n",
    "print(\"👽 It's okay, no biggie.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3db913df",
   "metadata": {},
   "source": [
    "# Q. train 데이터와 test 데이터에 정규화를 적용해봅시다.\n",
    "train_min = train.min()\n",
    "train_max = train.max()\n",
    "\n",
    "# 중요한 점은, test 데이터에 min-max scaling을 적용할 때도\n",
    "# train 데이터 기준으로 수행해야 한다는 것입니다.\n",
    "train_min_max = (train - train_min) / (train_max - train_min)\n",
    "test_min_max = (test - train_min) / (train_max - train_min)\n",
    "\n",
    "print(\"💫 It's okay, no biggie...\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be667962",
   "metadata": {},
   "source": [
    "train_min_max"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f919e226",
   "metadata": {},
   "source": [
    "test_min_max"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d9409a6",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train = [[10, -10], [30, 10], [50, 0]]\n",
    "test = [[0, 1]]\n",
    "scaler = MinMaxScaler()\n",
    "print(\"👽 It's okay, no biggie.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9a7e9a4",
   "metadata": {},
   "source": [
    "scaler.fit_transform(train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "51181677",
   "metadata": {},
   "source": [
    " 로그 변환 등의 기법도 정규화와 함께 사용하면 도움이 될 수 있다는 것도 기억해 주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac26731",
   "metadata": {},
   "source": [
    "## 원-핫 인코딩(One-Hot Encoding)\n",
    "이제 범주형 데이터인 국가명 컬럼을 다뤄보도록 하겠습니다.\n",
    "머신러닝이나 딥러닝 프레임워크에서 범주형을 지원하지 않는 경우 원-핫 인코딩을 해야 합니다.\n",
    "\n",
    "원-핫 인코딩이란 카테고리별 이진 특성을 만들어 해당하는 특성만 1, 나머지는 0으로 만드는 방법입니다. 그럼, pandas로 국가명 컬럼을 원-핫 인코딩을 해보겠습니다.\n",
    "\n",
    "pandas에서 get_dummies 함수를 통해 손쉽게 원-핫 인코딩을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9a85ee5",
   "metadata": {},
   "source": [
    "#trade 데이터의 국가명 컬럼 원본\n",
    "print(trade['국가명'].head())  \n",
    "\n",
    "# get_dummies를 통해 국가명 원-핫 인코딩\n",
    "country = pd.get_dummies(trade['국가명'])\n",
    "country.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86b41348",
   "metadata": {},
   "source": [
    "trade = pd.concat([trade, country], axis=1)\n",
    "trade.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9ad803",
   "metadata": {},
   "source": [
    "## 구간화(Binning)\n",
    "지금까지 trade 데이터를 다루면서 다양 전처리 기법을 배웠습니다. 이제 다른 전처리 기법을 배워보도록 하겠습니다.\n",
    "\n",
    "salary에 소득 데이터가 있다고 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dde29d9c",
   "metadata": {},
   "source": [
    "salary = pd.Series([4300, 8370, 1750, 3830, 1840, 4220, 3020, 2290, 4740, 4600, \n",
    "                    2860, 3400, 4800, 4470, 2440, 4530, 4850, 4850, 4760, 4500, \n",
    "                    4640, 3000, 1880, 4880, 2240, 4750, 2750, 2810, 3100, 4290, \n",
    "                    1540, 2870, 1780, 4670, 4150, 2010, 3580, 1610, 2930, 4300, \n",
    "                    2740, 1680, 3490, 4350, 1680, 6420, 8740, 8980, 9080, 3990, \n",
    "                    4960, 3700, 9600, 9330, 5600, 4100, 1770, 8280, 3120, 1950, \n",
    "                    4210, 2020, 3820, 3170, 6330, 2570, 6940, 8610, 5060, 6370,\n",
    "                    9080, 3760, 8060, 2500, 4660, 1770, 9220, 3380, 2490, 3450, \n",
    "                    1960, 7210, 5810, 9450, 8910, 3470, 7350, 8410, 7520, 9610, \n",
    "                    5150, 2630, 5610, 2750, 7050, 3350, 9450, 7140, 4170, 3090])\n",
    "print(\"👽 Almost there..\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6452492",
   "metadata": {},
   "source": [
    "salary.hist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acaa83ae",
   "metadata": {},
   "source": [
    "bins = [0, 2000, 4000, 6000, 8000, 10000]\n",
    "print(\"👽 Almost there..\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a4d884d",
   "metadata": {},
   "source": [
    "ctg = pd.cut(salary, bins=bins)\n",
    "ctg"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "352f062f",
   "metadata": {},
   "source": [
    "print('salary[0]:', salary[0])\n",
    "print('salary[0]가 속한 카테고리:', ctg[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a932565c",
   "metadata": {},
   "source": [
    "ctg.value_counts().sort_index()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b12d84ad",
   "metadata": {},
   "source": [
    "이렇게 특정 구간을 지정해 줘도 되고, 구간의 개수를 지정해 줄 수도 있습니다. bins 옵션에 정수를 입력하면 데이터의 최솟값에서 최댓값을 균등하게 bins 개수만큼 나눠줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2363e99",
   "metadata": {},
   "source": [
    "# Q. 'bins' 옵션에 6을 입력해서 cut() 함수를 사용해보세요.\n",
    "ctg = pd.cut(salary, bins=6)\n",
    "ctg"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7c9274a9",
   "metadata": {},
   "source": [
    "`qcut`은 구간을 일정하게 나누는 것이 아니라 데이터의 분포를 비슷한 크기의 그룹으로 나눠줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "375a5873",
   "metadata": {},
   "source": [
    "ctg = pd.qcut(salary, q=5)\n",
    "ctg"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f67bb2d4",
   "metadata": {},
   "source": [
    "print(ctg.value_counts().sort_index())\n",
    "print(\".\\n.\\n🛸 Well done!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "08b35918",
   "metadata": {},
   "source": [
    "마무리와 복습 과제\n",
    "현실에서 여러분이 만날 데이터는 깨끗하지 않은 경우가 많습니다. 그래서 오늘은 표 데이터 전처리하는 방법을 배웠습니다. 어떤 내용을 다뤘는지 키워드로 살펴보겠습니다.\n",
    "\n",
    "    결측치(Missing Data)\n",
    "    중복된 데이터\n",
    "    이상치(Outlier)\n",
    "    정규화(Normalization)\n",
    "    원-핫 인코딩(One-Hot Encoding)\n",
    "    구간화(Binning)\n",
    "위의 내용을 배웠지만 모든 데이터에 일괄적으로 적용해야 하는 것은 아닙니다. 데이터 전처리는 데이터의 특성을 파악해서 해야 합니다. 그래서 다소 번거롭게 느껴질 수 있지만 꼭 해야 하는 과정인 만큼 복습해 보시기를 권합니다. 남는 시간 동안 아래 데이터셋의 전처리 과정을 진행해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70388341",
   "metadata": {},
   "source": [
    "`vgsales.csv`\n",
    "들어가며 스텝에서 정상적으로 심볼릭 링크를 거셨다면 이미 여러분의 실습 폴더 ~/aiffel/data_preprocess/data 로 옮겨져있을 것입니다.\n",
    "\n",
    "위 데이터의 출처는 캐글의 Video Game Sales 데이터셋입니다. 16,500개 이상의 비디오 게임에 대한 매출 정보 등이 담겨 있습니다. 혹시 게임을 좋아하시는 분께는 아주 흥미로운 데이터셋이 될지도 모르겠습니다. 위에 언급된 전처리 기법 중 다수를 시도해 볼 수 있는 데이터셋이므로, 오늘 배운 기법들이 본인의 것이 될 수 있도록 복습을 통해 다시 한번 개념을 다져보시기를 권합니다.\n",
    "\n",
    "이외에도 여러분이 가진 표 데이터를 직접 전처리해 보시면 더욱 흥미롭게 하실 수 있습니다. 데이터가 없다면 공공데이터포털, 캐글에서 관심 있는 분야의 데이터를 찾아보시기 바랍니다.\n",
    "\n",
    "수고하셨습니다! 🛸"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 수치형 데이터의 결측치를 채우는 방법\n",
    "① 특정 값을 지정해 줄 수 있습니다. 그러나 결측치가 많은 경우, 모두 같은 값으로 대체한다면 데이터의 분산이 실제보다 작아지는 문제가 생길 수 있습니다.\n",
    "② 평균, 중앙값 등으로 대체할 수 있습니다. 1번에서 특정 값으로 대체했을 때와 마찬가지로 결측치가 많은 경우 데이터의 분산이 실제보다 작아지는 문제가 발생할 수 있습니다.\n",
    "③ 다른 데이터를 이용해 예측값으로 대체할 수 있습니다. 예를 들어 머신러닝 모델로 예측값을 만들고, 이 값으로 결측치를 보완할 수 있습니다.\n",
    "④ 시계열 특성을 가진 데이터의 경우 앞뒤 데이터를 통해 결측치를 대체할 수 있습니다. 예를 들어 기온을 측정하는 센서 데이터에서 결측치가 발생할 경우, 전후 데이터의 평균으로 보완할 수 있습니다.\n",
    "① 특정 값을 지정해 줄 수 있습니다. 그러나 결측치가 많은 경우, 모두 같은 값으로 대체한다면 데이터의 분산이 실제보다 작아지는 문제가 생길 수 있습니다.\n",
    "② 평균, 중앙값 등으로 대체할 수 있습니다. 1번에서 특정 값으로 대체했을 때와 마찬가지로 결측치가 많은 경우 데이터의 분산이 실제보다 작아지는 문제가 발생할 수 있습니다.\n",
    "③ 다른 데이터를 이용해 예측값으로 대체할 수 있습니다. 예를 들어 머신러닝 모델로 예측값을 만들고, 이 값으로 결측치를 보완할 수 있습니다.\n",
    "④ 시계열 특성을 가진 데이터의 경우 앞뒤 데이터를 통해 결측치를 대체할 수 있습니다. 예를 들어 기온을 측정하는 센서 데이터에서 결측치가 발생할 경우, 전후 데이터의 평균으로 보완할 수 있습니다."
   ],
   "id": "b4514f92ebc3a835"
  },
  {
   "cell_type": "markdown",
   "id": "3e879839",
   "metadata": {},
   "source": [
    "### Q. 원-핫 인코딩은 분류(classification) 문제를 해결하는 딥러닝 모델을 구현할 때도 많이 사용됩니다. 예를 들어 0부터 9까지의 손글씨 숫자를 인식하는 모델이 있죠. 0부터 9까지의 카테고리를 원-핫 인코딩으로 표현할 때, 4와 7의 원-핫 인코딩 결과는 어떻게 될까요?(원-핫 인코딩 결과의 각 특성은 순서대로 0부터 9에 해당된다고 가정합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efda485",
   "metadata": {},
   "source": [
    "4 → [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "7 → [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
